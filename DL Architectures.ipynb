{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc9177",
   "metadata": {},
   "source": [
    "<h3>üß† Types of Neural Network Architectures</h3>\n",
    "\n",
    "<div style=\"font-size: 12px; width: 90%;\">\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Type</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Params</th>\n",
    "      <th>Train Time</th>\n",
    "      <th>Complexity</th>\n",
    "      <th>Applications</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Shallow</td>\n",
    "      <td>1‚Äì3</td>\n",
    "      <td>1K‚Äì10K</td>\n",
    "      <td>Minutes‚ÄìHours</td>\n",
    "      <td>Simple</td>\n",
    "      <td>Linear reg; Binary cls; Feature extract</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Medium</td>\n",
    "      <td>4‚Äì10</td>\n",
    "      <td>100K‚Äì1M</td>\n",
    "      <td>Hours‚ÄìDays</td>\n",
    "      <td>Moderate</td>\n",
    "      <td>Image cls (CNN); LM (RNN); Speech recog</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Deep</td>\n",
    "      <td>11‚Äì50</td>\n",
    "      <td>1M‚Äì10M</td>\n",
    "      <td>Days‚ÄìWeeks</td>\n",
    "      <td>High</td>\n",
    "      <td>Obj detect (YOLO); Seg (FCN); NLP</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Very Deep</td>\n",
    "      <td>51‚Äì100+</td>\n",
    "      <td>10M‚Äì100M+</td>\n",
    "      <td>Weeks‚ÄìMonths</td>\n",
    "      <td>Very High</td>\n",
    "      <td>SOTA vision; Adv NLP (QA, MT)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbbd8b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px;\">‚úÖ <b>Universal Approximation Theorem</b>: A shallow neural network (1 hidden layer) can approximate any complex function given enough neurons & a proper activation.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f41214",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:16px; margin-bottom:6px;\">üì¶ Deep Learning Architectures</h3>\n",
    "\n",
    "<!-- ANN Architectures -->\n",
    "<b style=\"font-size:14px;\">1. ANN Architectures</b>\n",
    "<table style=\"font-size:13px; font-family:sans-serif; border-collapse: collapse; margin-bottom: 12px;\">\n",
    "  <tr><td><b>McCulloch-Pitts</b></td><td>Binary threshold model.</td></tr>\n",
    "  <tr><td><b>Hebbian Network</b></td><td>‚ÄúNeurons that fire together, wire together.‚Äù</td></tr>\n",
    "  <tr><td><b>Perceptron / MLP</b></td><td>Basic/stacked feedforward layers.</td></tr>\n",
    "  <tr><td><b>ADALINE / MADALINE</b></td><td>Linear adaptive models.</td></tr>\n",
    "  <tr><td><b>Backpropagation</b></td><td>Error-driven learning for MLPs.</td></tr>\n",
    "  <tr><td><b>RBF Networks</b></td><td>Use Gaussian activations.</td></tr>\n",
    "</table>\n",
    "\n",
    "<!-- Vision Models -->\n",
    "<b style=\"font-size:14px;\">2. Vision Models</b>\n",
    "<table style=\"font-size:13px; font-family:sans-serif; border-collapse: collapse; margin-bottom: 12px;\">\n",
    "  <tr><td><b>Image Classification</b></td><td>Label entire image.</td></tr>\n",
    "  <tr><td><b>Object Detection</b></td><td>Locate + classify objects.</td></tr>\n",
    "  <tr><td><b>Image Segmentation</b></td><td>Semantic, Instance, Panoptic.</td></tr>\n",
    "</table>\n",
    "\n",
    "<!-- NLP Models -->\n",
    "<b style=\"font-size:14px;\">3. NLP Models</b>\n",
    "<table style=\"font-size:13px; font-family:sans-serif; border-collapse: collapse; margin-bottom: 12px;\">\n",
    "  <tr><td><b>RNN Family</b></td><td>RNN, LSTM, GRU, etc.</td></tr>\n",
    "  <tr><td><b>Transformer</b></td><td>Self-attention architecture.</td></tr>\n",
    "  <tr><td><b>Pretrained</b></td><td>BERT, RoBERTa, T5...</td></tr>\n",
    "  <tr><td><b>LLMs</b></td><td>GPT, LLaMA ‚Äî autoregressive.</td></tr>\n",
    "</table>\n",
    "\n",
    "<!-- Generative Models -->\n",
    "<b style=\"font-size:14px;\">4. Generative Models</b>\n",
    "<table style=\"font-size:13px; font-family:sans-serif; border-collapse: collapse;\">\n",
    "  <tr><td><b>Autoencoders</b></td><td>AE, VAE ‚Äî latent encoding.</td></tr>\n",
    "  <tr><td><b>GANs</b></td><td>Adversarial generation.</td></tr>\n",
    "  <tr><td><b>Flow Models</b></td><td>RealNVP, Glow.</td></tr>\n",
    "  <tr><td><b>Diffusion</b></td><td>DDPM, Stable Diffusion.</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70430e69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd69e51",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:20px;\">üß† NLP Model Timeline (1986‚Äì2025)</h2>\n",
    "\n",
    "<table style=\"font-size:13px; font-family:sans-serif; border-collapse: collapse; margin-bottom: 16px;\">\n",
    "  <tr><td><b>1986</b></td><td>RNN ‚Äì Basic sequential modeling</td></tr>\n",
    "  <tr><td><b>1997</b></td><td>LSTM ‚Äì Long-term memory via gating</td></tr>\n",
    "  <tr><td><b>2014</b></td><td>GRU, Bi-RNN, Stacked RNN, Seq2Seq, Additive Attention (Bahdanau)</td></tr>\n",
    "  <tr><td><b>2015</b></td><td>Multiplicative Attention (Luong)</td></tr>\n",
    "  <tr><td><b>2017</b></td><td>Transformer ‚Äì Self & Cross Attention</td></tr>\n",
    "  <tr><td><b>2018</b></td><td>BERT, GPT‚Äë1</td></tr>\n",
    "  <tr><td><b>2019</b></td><td>RoBERTa, DistilBERT, ALBERT, XLNet, ERNIE, GPT‚Äë2, MarianMT, BART</td></tr>\n",
    "  <tr><td><b>2020</b></td><td>T5/mT5, ELECTRA, DeBERTa, PEGASUS, ViT, Reformer, Linformer, Performer, BigBird, Longformer</td></tr>\n",
    "  <tr><td><b>2021</b></td><td>Mistral 7B, Switch Transformer, CLIP</td></tr>\n",
    "  <tr><td><b>2022</b></td><td>FlashAttention, BLIP, Flamingo, Perceiver IO</td></tr>\n",
    "  <tr><td><b>2023</b></td><td>GPT‚Äë4, LLaMA‚Äë1/2, DeepSeek (LLM + Coder)</td></tr>\n",
    "  <tr><td><b>2024</b></td><td>LLaMA‚Äë3.0 (Apr), LLaMA‚Äë3.1 / Mistral L2 (Jul), DeepSeek‚ÄëV3 (Dec), R1‚ÄëLite (Nov)</td></tr>\n",
    "  <tr><td><b>2025</b></td><td>Jan: DeepSeek‚ÄëR1, Mar: V3‚Äë0324, May: Devstral (Small‚Äë2505), Mixtral (MoE)</td></tr>\n",
    "</table>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"font-size:16px;\">üîπ 1. RNN Family ‚Äì Early Sequence Models</h3>\n",
    "<p><i>Goal:</i> Handle sequential data by using memory of past tokens.</p>\n",
    "\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>RNN:</b> Learns temporal patterns, but suffers from vanishing gradients.</li>\n",
    "  <li><b>LSTM:</b> Adds gates (input, forget, output) for long-term memory.</li>\n",
    "  <li><b>GRU:</b> Uses update/reset gates; simpler than LSTM.</li>\n",
    "  <li><b>Bidirectional RNN:</b> Processes sequences in both directions.</li>\n",
    "  <li><b>Stacked RNN:</b> Multiple RNN layers stacked for deeper features.</li>\n",
    "  <li><b>Encoder-Decoder:</b> Maps input ‚Üí output sequences, used in translation.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"font-size:16px;\">üîπ 2. Attention Mechanisms ‚Äì Beyond RNNs</h3>\n",
    "<p><i>Goal:</i> Focus on relevant input parts during prediction.</p>\n",
    "\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>Additive Attention:</b> Feedforward-based alignment (Bahdanau).</li>\n",
    "  <li><b>Multiplicative Attention:</b> Dot-product based (Luong).</li>\n",
    "  <li><b>Cross-Attention:</b> Decoder attends to encoder outputs.</li>\n",
    "  <li><b>Self-Attention:</b> Token attends to all others; base of Transformers.</li>\n",
    "  <li><b>Flash Attention:</b> Fast, memory-efficient self-attention.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"font-size:16px;\">üîπ 3. Transformer Era ‚Äì Scalable Parallel Processing</h3>\n",
    "<p><i>Goal:</i> Leverage parallelism & self-attention to scale beyond RNNs.</p>\n",
    "\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>Transformer (2017):</b> Encoder-Decoder, multi-head, positional encoding.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Efficient Transformers</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>BigBird / Longformer:</b> Sparse/windowed attention for long texts.</li>\n",
    "  <li><b>Reformer:</b> Memory-efficient using hashing + reversible layers.</li>\n",
    "  <li><b>Switch Transformer:</b> MoE-based dynamic routing.</li>\n",
    "  <li><b>Performer / Linformer:</b> Linear-time attention.</li>\n",
    "  <li><b>Flash Attention:</b> GPU-optimized fast attention.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Vision & Multimodal Transformers</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>ViT:</b> Transformer on image patches.</li>\n",
    "  <li><b>Perceiver IO:</b> Handles text, vision, audio.</li>\n",
    "  <li><b>CLIP / BLIP / Flamingo:</b> Vision+language tasks.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"font-size:16px;\">üîπ 4. Pretrained Transformers ‚Äì Generalizable Models</h3>\n",
    "<p><i>Goal:</i> Learn general-purpose representations via self-supervised training.</p>\n",
    "\n",
    "<b>Encoder-only</b>  \n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>BERT:</b> Masked LM; bidirectional context.</li>\n",
    "  <li><b>RoBERTa:</b> BERT+ longer training, more data.</li>\n",
    "  <li><b>DistilBERT:</b> Compressed version with 95% accuracy.</li>\n",
    "  <li><b>ALBERT:</b> Lightweight BERT with parameter sharing.</li>\n",
    "  <li><b>DeBERTa:</b> Disentangled attention mechanism.</li>\n",
    "  <li><b>ELECTRA:</b> Detects replaced tokens (generator-discriminator).</li>\n",
    "</ul>\n",
    "\n",
    "<b>Encoder-Decoder</b>  \n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>T5 / mT5:</b> Text-to-text for all tasks (multilingual).</li>\n",
    "  <li><b>PEGASUS:</b> Summarization-focused pretraining.</li>\n",
    "  <li><b>BART:</b> Denoising + sequence-to-sequence generation.</li>\n",
    "  <li><b>MarianMT:</b> Efficient multilingual translator.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hybrid</b>  \n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>XLNet:</b> Combines autoregressive and autoencoding.</li>\n",
    "  <li><b>ERNIE:</b> Integrates structured knowledge (e.g., KG).</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"font-size:16px;\">üîπ 5. Generative Models ‚Äì Foundation of LLMs</h3>\n",
    "<p><i>Goal:</i> Autoregressive generation of fluent, coherent text.</p>\n",
    "\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>GPT-1:</b> Decoder-only Transformer.</li>\n",
    "  <li><b>GPT-2:</b> Large-scale, paragraph coherence.</li>\n",
    "  <li><b>GPT-3:</b> 175B params; prompt learning.</li>\n",
    "  <li><b>GPT-4:</b> Multimodal, highly aligned reasoning.</li>\n",
    "  <li><b>Open-Source:</b> LLaMA, Mistral, DeepSeek, Falcon.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0628b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfbd3538",
   "metadata": {},
   "source": [
    "<h2>üß† Computer Vision Model Architectures</h2>\n",
    "\n",
    "<b>üìå Computer Vision Tasks</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>Image Classification</b></li>\n",
    "  <li><b>Object Detection</b>: Localization + Classification</li>\n",
    "  <li><b>Image Segmentation</b>: \n",
    "    <ul>\n",
    "      <li>Semantic Segmentation</li>\n",
    "      <li>Instance Segmentation</li>\n",
    "      <li>Panoptic Segmentation</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><b>Landmark Detection / Face Recognition</b></li>\n",
    "  <li><b>OCR (Optical Character Recognition)</b>: Text Extraction</li>\n",
    "  <li><b>Visual Tracking</b></li>\n",
    "  <li><b>Action Recognition / Pose Estimation</b></li>\n",
    "  <li><b>3D Construction</b></li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>üß† What is CNN?</h3>\n",
    "<p style=\"font-size:13px;\">\n",
    "CNN (Convolutional Neural Networks) are partially connected networks that reduce the number of parameters and speed up training.\n",
    "</p>\n",
    "\n",
    "<b>üî∏ Layers in CNN</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>Convolution Layer</b>: Applies learnable filters to extract features.</li>\n",
    "  <li><b>ReLU Layer</b>: Introduces non-linearity.</li>\n",
    "  <li><b>Max Pooling Layer</b>: Reduces dimensionality.</li>\n",
    "  <li><b>Fully Connected Layer</b>: Connects all neurons from previous layers.</li>\n",
    "</ul>\n",
    "\n",
    "<b>üî∏ Benefits of ReLU</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li><b>Non-Linearity</b>: Learns complex relationships.</li>\n",
    "  <li><b>Sparse Outputs</b>: Reduces computation.</li>\n",
    "  <li><b>Computational Efficiency</b>: Faster than other activations.</li>\n",
    "  <li><b>Easy to Optimize</b>: Aids optimizer convergence.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:13px;\"><b>Max Pooling Formula</b>: (n + 2p - f) / s + 1</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>üìä Image Classification Architectures</h3>\n",
    "\n",
    "<b>üîπ Basic CNN Models</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>AlexNet-8 (2012)</li>\n",
    "  <li>VGGNet-16, VGGNet-19 (2014)</li>\n",
    "  <li>Inception v1, v2, v3, v4 (GoogleNet, 2014)</li>\n",
    "  <li>ResNet (2015)</li>\n",
    "  <li>ConvNext (2022)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Advanced CNN Models</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>DenseNet (2016)</li>\n",
    "  <li>SqueezeNet (2016)</li>\n",
    "  <li>ShuffleNet (2017)</li>\n",
    "  <li>MobileNet (2017)</li>\n",
    "  <li>Xception (2017)</li>\n",
    "  <li>EfficientNet (2019)</li>\n",
    "  <li>EfficientNet-V2 (2021)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Neural Architecture Search Models</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>NASNet (2018)</li>\n",
    "  <li>MNASNet (2019)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Vision Transformers</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>Vision Transformer (ViT, 2020)</li>\n",
    "  <li>DEVit (2020)</li>\n",
    "  <li>Swin Transformer (2021)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üìä Comparison of Models</b>\n",
    "<table style=\"font-size:13px; border-collapse:collapse;\">\n",
    "  <tr><th>Model</th><th>Best Use Case</th></tr>\n",
    "  <tr><td>CNN</td><td>Small datasets, quick implementation</td></tr>\n",
    "  <tr><td>Advanced CNNs</td><td>High accuracy, transfer learning</td></tr>\n",
    "  <tr><td>Neural Architecture Search</td><td>Computational optimization</td></tr>\n",
    "  <tr><td>Vision Transformer</td><td>Large datasets, rich relationships</td></tr>\n",
    "</table>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>üß™ Tasks in Image Classification</h3>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>Binary Classification</li>\n",
    "  <li>Multi-Class Classification</li>\n",
    "  <li>Multi-Label Classification</li>\n",
    "  <li>Facial Recognition (Haar Cascade)</li>\n",
    "  <li>Image Retrieval</li>\n",
    "  <li>Image Tagging</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>üéØ Object Detection</h3>\n",
    "\n",
    "<b>üîπ Single-Stage Detectors</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>YOLO (v1‚Äìv8)</li>\n",
    "  <li>YOLO-NAS (2023)</li>\n",
    "  <li>SSD, DSSD (2016)</li>\n",
    "  <li>RetinaNet (2017)</li>\n",
    "  <li>EfficientDet (2020)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Two-Stage Detectors</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>R-CNN (2014)</li>\n",
    "  <li>Fast R-CNN (2015)</li>\n",
    "  <li>Faster R-CNN (2015)</li>\n",
    "  <li>Mask R-CNN (2017)</li>\n",
    "  <li>Cascade R-CNN (2018)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Transformer-Based Detectors</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>DETR (2020)</li>\n",
    "  <li>Deformable DETR (2021)</li>\n",
    "  <li>Swin Transformer (2021)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Real-Time Detectors</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>YOLO (v1‚Äìv8)</li>\n",
    "  <li>SSD (2016)</li>\n",
    "  <li>EfficientDet (2020)</li>\n",
    "  <li>RT-DETR (2023)</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>üß© Image Segmentation Models</h3>\n",
    "\n",
    "<b>üîπ CNN-Based Segmentation</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>FCN (2015)</li>\n",
    "  <li>U-Net (2015)</li>\n",
    "  <li>SegNet (2016)</li>\n",
    "  <li>PSPNet (2017)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Region-Based Segmentation</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>Mask R-CNN (2017)</li>\n",
    "  <li>DeepLab v1‚Äìv3+ (2015‚Äì2018)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Transformer-Based Segmentation</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>SETR (2021)</li>\n",
    "  <li>Swin-Unet (2021)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ 3D Segmentation</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>VoxResNet (2016)</li>\n",
    "  <li>V-Net (2016)</li>\n",
    "  <li>3D U-Net (2016)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üîπ Real-Time Segmentation</b>\n",
    "<ul style=\"font-size:13px;\">\n",
    "  <li>ENet (2016)</li>\n",
    "  <li>BiSeNet (2018)</li>\n",
    "  <li>Fast-SCNN (2019)</li>\n",
    "</ul>\n",
    "\n",
    "<b>üìä Segmentation Model Comparison</b>\n",
    "<table style=\"font-size:13px; border-collapse:collapse;\">\n",
    "  <tr><th>Model</th><th>Best Use Case</th></tr>\n",
    "  <tr><td>CNN-Based</td><td>Efficient 2D segmentation, medical imaging</td></tr>\n",
    "  <tr><td>Region-Based</td><td>High accuracy for complex objects</td></tr>\n",
    "  <tr><td>Transformer-Based</td><td>Large datasets, contextual understanding</td></tr>\n",
    "  <tr><td>3D Segmentation</td><td>MRI, CT scan analysis</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cbdea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
