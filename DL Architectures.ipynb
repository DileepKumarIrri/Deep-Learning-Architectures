{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc9177",
   "metadata": {},
   "source": [
    "<h3>üß† Types of Neural Network Architectures</h3>\n",
    "\n",
    "<div style=\"font-size: 12px; width: 90%;\">\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Type</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Params</th>\n",
    "      <th>Train Time</th>\n",
    "      <th>Complexity</th>\n",
    "      <th>Applications</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Shallow</td>\n",
    "      <td>1‚Äì3</td>\n",
    "      <td>1K‚Äì10K</td>\n",
    "      <td>Minutes‚ÄìHours</td>\n",
    "      <td>Simple</td>\n",
    "      <td>Linear reg; Binary cls; Feature extract</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Medium</td>\n",
    "      <td>4‚Äì10</td>\n",
    "      <td>100K‚Äì1M</td>\n",
    "      <td>Hours‚ÄìDays</td>\n",
    "      <td>Moderate</td>\n",
    "      <td>Image cls (CNN); LM (RNN); Speech recog</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Deep</td>\n",
    "      <td>11‚Äì50</td>\n",
    "      <td>1M‚Äì10M</td>\n",
    "      <td>Days‚ÄìWeeks</td>\n",
    "      <td>High</td>\n",
    "      <td>Obj detect (YOLO); Seg (FCN); NLP</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Very Deep</td>\n",
    "      <td>51‚Äì100+</td>\n",
    "      <td>10M‚Äì100M+</td>\n",
    "      <td>Weeks‚ÄìMonths</td>\n",
    "      <td>Very High</td>\n",
    "      <td>SOTA vision; Adv NLP (QA, MT)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbbd8b",
   "metadata": {},
   "source": [
    "‚úÖ **Universal Approximation Theorem**:  \n",
    "A shallow neural network (with one hidden layer) can mimic any complex function accurately, if it has enough neurons and a suitable activation¬†function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f41214",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size:16px; margin-bottom:6px;\">üì¶ Deep Learning Architectures</h3>\n",
    "\n",
    "<style>\n",
    "  .compact-grid {\n",
    "    display: grid;\n",
    "    grid-template-columns: 160px auto;\n",
    "    row-gap: 2px;\n",
    "    font-size: 13px;\n",
    "    font-family: sans-serif;\n",
    "  }\n",
    "  .compact-grid b {\n",
    "    white-space: nowrap;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<b style=\"font-size:14px;\">1. ANN Architectures</b>\n",
    "<div class=\"compact-grid\">\n",
    "  <b>McCulloch-Pitts</b> <span>Binary threshold model.</span>\n",
    "  <b>Hebbian Network</b> <span>‚ÄúNeurons that fire together, wire together.‚Äù</span>\n",
    "  <b>Perceptron / MLP</b> <span>Basic/stacked feedforward layers.</span>\n",
    "  <b>ADALINE / MADALINE</b> <span>Linear adaptive models.</span>\n",
    "  <b>Backpropagation</b> <span>Error-driven learning for MLPs.</span>\n",
    "  <b>RBF Networks</b> <span>Use Gaussian activations.</span>\n",
    "</div>\n",
    "\n",
    "<b style=\"font-size:14px;\">2. Vision Models</b>\n",
    "<div class=\"compact-grid\">\n",
    "  <b>Image Classification</b> <span>Label entire image.</span>\n",
    "  <b>Object Detection</b> <span>Locate + classify objects.</span>\n",
    "  <b>Image Segmentation</b> <span>Semantic, Instance, Panoptic.</span>\n",
    "</div>\n",
    "\n",
    "<b style=\"font-size:14px;\">3. NLP Models</b>\n",
    "<div class=\"compact-grid\">\n",
    "  <b>RNN Family</b> <span>RNN, LSTM, GRU, etc.</span>\n",
    "  <b>Transformer</b> <span>Self-attention architecture.</span>\n",
    "  <b>Pretrained</b> <span>BERT, RoBERTa, T5...</span>\n",
    "  <b>LLMs</b> <span>GPT, LLaMA ‚Äî autoregressive.</span>\n",
    "</div>\n",
    "\n",
    "<b style=\"font-size:14px;\">4. Generative Models</b>\n",
    "<div class=\"compact-grid\">\n",
    "  <b>Autoencoders</b> <span>AE, VAE ‚Äî latent encoding.</span>\n",
    "  <b>GANs</b> <span>Adversarial generation.</span>\n",
    "  <b>Flow Models</b> <span>RealNVP, Glow.</span>\n",
    "  <b>Diffusion</b> <span>DDPM, Stable Diffusion.</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6b070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801fe3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d86def",
   "metadata": {},
   "source": [
    "<h2>üß† NLP Model Timeline (1986‚Äì2025)</h2>\n",
    "<style>\n",
    "  .timeline {\n",
    "    display: grid;\n",
    "    grid-template-columns: 70px auto;\n",
    "    row-gap: 4px;\n",
    "    font-family: sans-serif;\n",
    "    font-size: 14px;\n",
    "  }\n",
    "  .year {\n",
    "    font-weight: bold;\n",
    "    white-space: nowrap;\n",
    "  }\n",
    "</style>\n",
    "<div class=\"timeline\">\n",
    "  <div class=\"year\">1986</div><div>RNN ‚Äì Basic sequential modeling</div>\n",
    "  <div class=\"year\">1997</div><div>LSTM ‚Äì Long-term memory via gating</div>\n",
    "  <div class=\"year\">2014</div><div>GRU, Bi-RNN, Stacked RNN, Seq2Seq, Additive Attention (Bahdanau)</div>\n",
    "  <div class=\"year\">2015</div><div>Multiplicative Attention (Luong)</div>\n",
    "  <div class=\"year\">2017</div><div>Transformer ‚Äì Self & Cross Attention</div>\n",
    "  <div class=\"year\">2018</div><div>BERT, GPT‚Äë1</div>\n",
    "  <div class=\"year\">2019</div><div>RoBERTa, DistilBERT, ALBERT, XLNet, ERNIE, GPT‚Äë2, MarianMT, BART</div>\n",
    "  <div class=\"year\">2020</div><div>T5/mT5, ELECTRA, DeBERTa, PEGASUS, ViT, Reformer, Linformer, Performer, BigBird, Longformer</div>\n",
    "  <div class=\"year\">2021</div><div>Mistral 7B, Switch Transformer, CLIP</div>\n",
    "  <div class=\"year\">2022</div><div>FlashAttention, BLIP, Flamingo, Perceiver IO</div>\n",
    "  <div class=\"year\">2023</div><div>GPT‚Äë4, LLaMA‚Äë1/2, DeepSeek (LLM + Coder)</div>\n",
    "  <div class=\"year\">2024</div><div>LLaMA‚Äë3.0 (Apr), LLaMA‚Äë3.1 / Mistral L2 (Jul), DeepSeek‚ÄëV3 (Dec), R1‚ÄëLite (Nov)</div>\n",
    "  <div class=\"year\">2025</div><div>Jan: DeepSeek‚ÄëR1, Mar: V3‚Äë0324, May: Devstral (Small‚Äë2505), Mixtral (MoE)</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fa27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a7f2a9",
   "metadata": {},
   "source": [
    "## üîπ 1. RNN Family ‚Äì Early Sequence Models  \n",
    "**Goal**: Handle sequential data by using memory of past tokens.\n",
    "\n",
    "- **RNN**: Recurrent Neural Network; learns temporal patterns, but suffers from vanishing gradients.  \n",
    "- **LSTM**: Long Short-Term Memory; introduces gates (input, forget, output) to retain long-term dependencies.  \n",
    "- **GRU**: Gated Recurrent Unit; simpler than LSTM, uses update/reset gates for efficiency.  \n",
    "- **Bidirectional RNN**: Processes data in both forward and backward directions to capture full context.  \n",
    "- **Stacked RNN**: Multiple RNN layers stacked to learn deeper temporal features.  \n",
    "- **Encoder-Decoder**: Sequence-to-sequence model that maps input to output sequences, used in translation.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Attention Mechanisms ‚Äì Beyond RNNs  \n",
    "**Goal**: Focus on relevant parts of the input sequence during prediction.\n",
    "\n",
    "- **Additive Attention (Bahdanau)**: Uses learnable weights via feedforward layers for alignment scoring.  \n",
    "- **Multiplicative Attention (Luong)**: Uses dot-product between query and key vectors; more efficient.  \n",
    "- **Cross-Attention**: Decoder queries encoder outputs, key for sequence-to-sequence models.  \n",
    "- **Self-Attention**: Each token attends to all others in the sequence; core to Transformers.  \n",
    "- **Flash Attention**: Highly optimized self-attention with reduced memory and faster runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Transformer Era ‚Äì Scalable Parallel Processing  \n",
    "**Goal**: Use self-attention and parallelism to scale better than RNNs.\n",
    "\n",
    "### üî∏ Standard Transformer (2017)\n",
    "- **Architecture**: Encoder-Decoder with self-attention; introduced positional encoding and multi-head attention.\n",
    "\n",
    "### üî∏ Efficient Transformers\n",
    "- **BigBird / Longformer**: Handle long sequences using sparse or windowed attention patterns.  \n",
    "- **Reformer**: Improves memory by replacing attention with hashing and reversible layers.  \n",
    "- **Switch Transformer**: Uses sparse Mixture-of-Experts (MoE) for efficient routing.  \n",
    "- **Performer / Linformer**: Reduce complexity from quadratic to linear in sequence length.  \n",
    "- **Flash Attention**: Memory-efficient GPU-optimized attention mechanism.\n",
    "\n",
    "### üî∏ Vision & Multimodal Transformers\n",
    "- **ViT**: Vision Transformer; applies transformer architecture to image patches.  \n",
    "- **Perceiver IO**: Can handle diverse modalities (text, image, audio) with a unified model.  \n",
    "- **CLIP / BLIP / Flamingo**: Combine vision and text for tasks like image captioning and retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Pretrained Transformers ‚Äì Generalizable Models  \n",
    "**Goal**: Use self-supervised learning at scale to generalize to many tasks.\n",
    "\n",
    "### üî∏ Encoder-only (Bidirectional)\n",
    "- **BERT**: Bidirectional masked language model for contextual word embeddings.  \n",
    "- **RoBERTa**: Robustly optimized BERT with more data and training steps.  \n",
    "- **DistilBERT**: Smaller, faster BERT with 95% of performance.  \n",
    "- **ALBERT**: Lightweight BERT with cross-layer parameter sharing.  \n",
    "- **DeBERTa**: Uses disentangled attention for better token representation.  \n",
    "- **ELECTRA**: Trains discriminator to detect replaced tokens instead of masking.\n",
    "\n",
    "### üî∏ Encoder-Decoder\n",
    "- **T5**: Text-to-Text Transfer Transformer; unifies all NLP tasks as text-to-text.  \n",
    "- **mT5**: Multilingual T5 supporting many languages.  \n",
    "- **PEGASUS**: Pretraining optimized for abstractive summarization.  \n",
    "- **BART**: Denoising autoencoder + seq2seq; good for text generation.  \n",
    "- **MarianMT**: Efficient multilingual translation model.\n",
    "\n",
    "### üî∏ Hybrid Models\n",
    "- **XLNet**: Combines autoregressive and autoencoding; learns all factor permutations.  \n",
    "- **ERNIE**: Injects structured knowledge from knowledge graphs into BERT-style models.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Generative Models ‚Äì Foundation of LLMs  \n",
    "**Goal**: Autoregressively generate high-quality and coherent text.\n",
    "\n",
    "- **GPT-1**: Introduced decoder-only transformer for language modeling.  \n",
    "- **GPT-2**: Large-scale generative model with coherent paragraph generation.  \n",
    "- **GPT-3**: Few-shot learning with 175B parameters; enabled prompt-based learning.  \n",
    "- **GPT-4**: Multimodal and highly aligned LLM with better reasoning.  \n",
    "- **Open-Source**: LLaMA, Mistral, DeepSeek, Falcon ‚Äî efficient LLM alternatives for research and deployment.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
