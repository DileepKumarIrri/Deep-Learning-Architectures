
# ðŸ§  NLP Model Timeline (1986â€“2025)

```html
<style>
  .timeline {
    display: grid;
    grid-template-columns: 70px auto;
    row-gap: 4px;
    font-family: sans-serif;
    font-size: 14px;
  }
  .year {
    font-weight: bold;
    white-space: nowrap;
  }
</style>
```

<div class="timeline">

**1986** â€” RNN â€“ Basic sequential modeling  
**1997** â€” LSTM â€“ Long-term memory via gating  
**2014** â€” GRU, Bi-RNN, Stacked RNN, Seq2Seq, Additive Attention (Bahdanau)  
**2015** â€” Multiplicative Attention (Luong)  
**2017** â€” Transformer â€“ Self & Cross Attention  
**2018** â€” BERT, GPTâ€‘1  
**2019** â€” RoBERTa, DistilBERT, ALBERT, XLNet, ERNIE, GPTâ€‘2, MarianMT, BART  
**2020** â€” T5/mT5, ELECTRA, DeBERTa, PEGASUS, ViT, Reformer, Linformer, Performer, BigBird, Longformer  
**2021** â€” Mistral 7B, Switch Transformer, CLIP  
**2022** â€” FlashAttention, BLIP, Flamingo, Perceiver IO  
**2023** â€” GPTâ€‘4, LLaMAâ€‘1/2, DeepSeek (LLM + Coder)  
**2024** â€” LLaMAâ€‘3.0 (Apr), LLaMAâ€‘3.1 / Mistral L2 (Jul), DeepSeekâ€‘V3 (Dec), R1â€‘Lite (Nov)  
**2025** â€” Jan: DeepSeekâ€‘R1, Mar: V3â€‘0324, May: Devstral (Smallâ€‘2505), Mixtral (MoE)

</div>

---

## ðŸ”¹ 1. RNN Family â€“ Early Sequence Models  
**Goal**: Handle sequential data by using memory of past tokens.

- **RNN**: Recurrent Neural Network; learns temporal patterns, but suffers from vanishing gradients.  
- **LSTM**: Long Short-Term Memory; introduces gates (input, forget, output) to retain long-term dependencies.  
- **GRU**: Gated Recurrent Unit; simpler than LSTM, uses update/reset gates for efficiency.  
- **Bidirectional RNN**: Processes data in both forward and backward directions to capture full context.  
- **Stacked RNN**: Multiple RNN layers stacked to learn deeper temporal features.  
- **Encoder-Decoder**: Sequence-to-sequence model that maps input to output sequences, used in translation.

---

## ðŸ”¹ 2. Attention Mechanisms â€“ Beyond RNNs  
**Goal**: Focus on relevant parts of the input sequence during prediction.

- **Additive Attention (Bahdanau)**: Uses learnable weights via feedforward layers for alignment scoring.  
- **Multiplicative Attention (Luong)**: Uses dot-product between query and key vectors; more efficient.  
- **Cross-Attention**: Decoder queries encoder outputs, key for sequence-to-sequence models.  
- **Self-Attention**: Each token attends to all others in the sequence; core to Transformers.  
- **Flash Attention**: Highly optimized self-attention with reduced memory and faster runtime.

---

## ðŸ”¹ 3. Transformer Era â€“ Scalable Parallel Processing  
**Goal**: Use self-attention and parallelism to scale better than RNNs.

### ðŸ”¸ Standard Transformer (2017)
- **Architecture**: Encoder-Decoder with self-attention; introduced positional encoding and multi-head attention.

### ðŸ”¸ Efficient Transformers
- **BigBird / Longformer**: Handle long sequences using sparse or windowed attention patterns.  
- **Reformer**: Improves memory by replacing attention with hashing and reversible layers.  
- **Switch Transformer**: Uses sparse Mixture-of-Experts (MoE) for efficient routing.  
- **Performer / Linformer**: Reduce complexity from quadratic to linear in sequence length.  
- **Flash Attention**: Memory-efficient GPU-optimized attention mechanism.

### ðŸ”¸ Vision & Multimodal Transformers
- **ViT**: Vision Transformer; applies transformer architecture to image patches.  
- **Perceiver IO**: Can handle diverse modalities (text, image, audio) with a unified model.  
- **CLIP / BLIP / Flamingo**: Combine vision and text for tasks like image captioning and retrieval.

---

## ðŸ”¹ 4. Pretrained Transformers â€“ Generalizable Models  
**Goal**: Use self-supervised learning at scale to generalize to many tasks.

### ðŸ”¸ Encoder-only (Bidirectional)
- **BERT**: Bidirectional masked language model for contextual word embeddings.  
- **RoBERTa**: Robustly optimized BERT with more data and training steps.  
- **DistilBERT**: Smaller, faster BERT with 95% of performance.  
- **ALBERT**: Lightweight BERT with cross-layer parameter sharing.  
- **DeBERTa**: Uses disentangled attention for better token representation.  
- **ELECTRA**: Trains discriminator to detect replaced tokens instead of masking.

### ðŸ”¸ Encoder-Decoder
- **T5**: Text-to-Text Transfer Transformer; unifies all NLP tasks as text-to-text.  
- **mT5**: Multilingual T5 supporting many languages.  
- **PEGASUS**: Pretraining optimized for abstractive summarization.  
- **BART**: Denoising autoencoder + seq2seq; good for text generation.  
- **MarianMT**: Efficient multilingual translation model.

### ðŸ”¸ Hybrid Models
- **XLNet**: Combines autoregressive and autoencoding; learns all factor permutations.  
- **ERNIE**: Injects structured knowledge from knowledge graphs into BERT-style models.

---

## ðŸ”¹ 5. Generative Models â€“ Foundation of LLMs  
**Goal**: Autoregressively generate high-quality and coherent text.

- **GPT-1**: Introduced decoder-only transformer for language modeling.  
- **GPT-2**: Large-scale generative model with coherent paragraph generation.  
- **GPT-3**: Few-shot learning with 175B parameters; enabled prompt-based learning.  
- **GPT-4**: Multimodal and highly aligned LLM with better reasoning.  
- **Open-Source**: LLaMA, Mistral, DeepSeek, Falcon â€” efficient LLM alternatives for research and deployment.

---
